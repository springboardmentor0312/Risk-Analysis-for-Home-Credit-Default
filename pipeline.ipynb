{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "471064cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dask[dataframe] in c:\\programdata\\anaconda3\\lib\\site-packages (2022.7.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]) (2022.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]) (22.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]) (0.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]) (6.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]) (1.2.0)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]) (2.0.0)\n",
      "Requirement already satisfied: pandas>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.18 in c:\\users\\smruti deshpande\\appdata\\roaming\\python\\python310\\site-packages (from dask[dataframe]) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.0->dask[dataframe]) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.0->dask[dataframe]) (2.8.2)\n",
      "Requirement already satisfied: locket in c:\\programdata\\anaconda3\\lib\\site-packages (from partd>=0.3.10->dask[dataframe]) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.0->dask[dataframe]) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dask[dataframe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "820ab2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "application_train.csv already exists.\n",
      "application_test.csv already exists.\n",
      "bureau.csv already exists.\n",
      "bureau_balance.csv already exists.\n",
      "credit_card_balance.csv already exists.\n",
      "installments_payments.csv already exists.\n",
      "previous_application.csv already exists.\n",
      "POS_CASH_balance.csv already exists.\n",
      "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
      "0      100002       1         Cash loans           M            N   \n",
      "1      100003       0         Cash loans           F            N   \n",
      "2      100004       0    Revolving loans           M            Y   \n",
      "3      100006       0         Cash loans           F            N   \n",
      "4      100007       0         Cash loans           M            N   \n",
      "\n",
      "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
      "0               Y             0          202500.0    406597.5      24700.5   \n",
      "1               N             0          270000.0   1293502.5      35698.5   \n",
      "2               Y             0           67500.0    135000.0       6750.0   \n",
      "3               Y             0          135000.0    312682.5      29686.5   \n",
      "4               Y             0          121500.0    513000.0      21865.5   \n",
      "\n",
      "   ...  FLAG_DOCUMENT_18 FLAG_DOCUMENT_19 FLAG_DOCUMENT_20 FLAG_DOCUMENT_21  \\\n",
      "0  ...                 0                0                0                0   \n",
      "1  ...                 0                0                0                0   \n",
      "2  ...                 0                0                0                0   \n",
      "3  ...                 0                0                0                0   \n",
      "4  ...                 0                0                0                0   \n",
      "\n",
      "  AMT_REQ_CREDIT_BUREAU_HOUR AMT_REQ_CREDIT_BUREAU_DAY  \\\n",
      "0                        0.0                       0.0   \n",
      "1                        0.0                       0.0   \n",
      "2                        0.0                       0.0   \n",
      "3                        NaN                       NaN   \n",
      "4                        0.0                       0.0   \n",
      "\n",
      "   AMT_REQ_CREDIT_BUREAU_WEEK  AMT_REQ_CREDIT_BUREAU_MON  \\\n",
      "0                         0.0                        0.0   \n",
      "1                         0.0                        0.0   \n",
      "2                         0.0                        0.0   \n",
      "3                         NaN                        NaN   \n",
      "4                         0.0                        0.0   \n",
      "\n",
      "   AMT_REQ_CREDIT_BUREAU_QRT  AMT_REQ_CREDIT_BUREAU_YEAR  \n",
      "0                        0.0                         1.0  \n",
      "1                        0.0                         0.0  \n",
      "2                        0.0                         0.0  \n",
      "3                        NaN                         NaN  \n",
      "4                        0.0                         0.0  \n",
      "\n",
      "[5 rows x 122 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import gdown\n",
    "import requests\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function to construct Google Drive direct download link\n",
    "def get_google_drive_url(file_id):\n",
    "    return f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Get file IDs from the .env file\n",
    "file_ids = {\n",
    "    \"application_train\": os.getenv(\"APPLICATION_TRAIN_ID\"),\n",
    "    \"application_test\": os.getenv(\"APPLICATION_TEST_ID\"),\n",
    "    \"bureau\": os.getenv(\"BUREAU_ID\"),\n",
    "    \"bureau_balance\": os.getenv(\"BUREAU_BALANCE_ID\"),\n",
    "    \"credit_card_balance\": os.getenv(\"CREDIT_CARD_BALANCE_ID\"),\n",
    "    \"installments_payments\": os.getenv(\"INSTALLMENTS_PAYMENTS_ID\"),\n",
    "    \"previous_application\": os.getenv(\"PREVIOUS_APPLICATION_ID\"),\n",
    "    \"POS_CASH_balance\": os.getenv(\"POS_CASH_BALANCE_ID\"),\n",
    "}\n",
    "\n",
    "# Construct direct download links\n",
    "google_drive_links = {key: get_google_drive_url(value) for key, value in file_ids.items()}\n",
    "\n",
    "# Function to download a file using gdown, only if not already downloaded\n",
    "def download_csv(file_url, output_path):\n",
    "    if not os.path.exists(output_path):  # Check if file already exists\n",
    "        print(f\"Downloading {output_path}...\")\n",
    "        try:\n",
    "            gdown.download(file_url, output_path, quiet=False)\n",
    "            print(f\"Downloaded {output_path}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error downloading {file_url}: {e}\")\n",
    "    else:\n",
    "        print(f\"{output_path} already exists.\")\n",
    "\n",
    "# Define the output file paths\n",
    "output_paths = {\n",
    "    \"application_train\": \"application_train.csv\",\n",
    "    \"application_test\": \"application_test.csv\",\n",
    "    \"bureau\": \"bureau.csv\",\n",
    "    \"bureau_balance\": \"bureau_balance.csv\",\n",
    "    \"credit_card_balance\": \"credit_card_balance.csv\",\n",
    "    \"installments_payments\": \"installments_payments.csv\",\n",
    "    \"previous_application\": \"previous_application.csv\",\n",
    "    \"POS_CASH_balance\": \"POS_CASH_balance.csv\"\n",
    "}\n",
    "\n",
    "# Download the datasets\n",
    "for key, file_url in google_drive_links.items():\n",
    "    download_csv(file_url, output_paths[key])\n",
    "\n",
    "# Load datasets from local files\n",
    "try:\n",
    "    app_train = dd.read_csv(output_paths[\"application_train\"], on_bad_lines='skip')\n",
    "    app_test = dd.read_csv(output_paths[\"application_test\"], on_bad_lines='skip')\n",
    "    bureau = dd.read_csv(output_paths[\"bureau\"], on_bad_lines='skip')\n",
    "    bureau_balance = dd.read_csv(output_paths[\"bureau_balance\"], on_bad_lines='skip')\n",
    "    credit_card_balance = dd.read_csv(output_paths[\"credit_card_balance\"], on_bad_lines='skip')\n",
    "    installments_payments = dd.read_csv(output_paths[\"installments_payments\"], on_bad_lines='skip')\n",
    "    previous_application = dd.read_csv(output_paths[\"previous_application\"], on_bad_lines='skip')\n",
    "    POS_CASH_balance = dd.read_csv(output_paths[\"POS_CASH_balance\"], on_bad_lines='skip')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV files: {e}\")\n",
    "\n",
    "# Example: Print the first few rows of the application_train dataset\n",
    "print(app_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3413cb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_BALANCE', 'AMT_ANNUITY', 'SK_DPD', 'CNT_CHILDREN', 'FLAG_OWN_CAR', 'CODE_GENDER', 'NAME_FAMILY_STATUS', 'NAME_INCOME_TYPE', 'NAME_HOUSING_TYPE', 'NAME_CONTRACT_TYPE']\n",
      "Pipeline saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Function to reduce memory usage\n",
    "def reduce_memory_usage(df):\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:  # Exclude string columns\n",
    "            if pd.api.types.is_integer_dtype(col_type):\n",
    "                df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "            elif pd.api.types.is_float_dtype(col_type):\n",
    "                df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    return df\n",
    "\n",
    "# Convert Dask DataFrames to Pandas DataFrames\n",
    "app_train = app_train.compute()\n",
    "app_test = app_test.compute()\n",
    "credit_card_balance = credit_card_balance.compute()\n",
    "\n",
    "# Reduce memory usage\n",
    "app_train = reduce_memory_usage(app_train)\n",
    "app_test = reduce_memory_usage(app_test)\n",
    "credit_card_balance = reduce_memory_usage(credit_card_balance)\n",
    "\n",
    "# Merge data\n",
    "columns_to_merge = ['SK_ID_CURR', 'AMT_BALANCE', 'SK_DPD']\n",
    "credit_card_balance_selected = credit_card_balance[columns_to_merge]\n",
    "\n",
    "app_train_dd = dd.from_pandas(app_train, npartitions=10)\n",
    "credit_card_balance_dd = dd.from_pandas(credit_card_balance_selected, npartitions=10)\n",
    "app_test_dd = dd.from_pandas(app_test[['SK_ID_CURR']], npartitions=10)\n",
    "\n",
    "merged_data_dd = dd.merge(app_train_dd, credit_card_balance_dd, on='SK_ID_CURR', how='left')\n",
    "merged_data_dd = dd.merge(merged_data_dd, app_test_dd, on='SK_ID_CURR', how='left')\n",
    "merged_data = merged_data_dd.compute()  # Convert the final Dask DataFrame to a Pandas DataFrame\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "merged_data['AMT_BALANCE'] = imputer.fit_transform(merged_data[['AMT_BALANCE']])\n",
    "merged_data['SK_DPD'] = SimpleImputer(strategy='median').fit_transform(merged_data[['SK_DPD']])\n",
    "\n",
    "# Map binary values to numeric\n",
    "binary_map = {'Y': 1, 'N': 0, 'M': 0, 'F': 1}\n",
    "merged_data['FLAG_OWN_CAR'] = merged_data['FLAG_OWN_CAR'].map(binary_map)\n",
    "merged_data['CODE_GENDER'] = merged_data['CODE_GENDER'].map(binary_map)\n",
    "\n",
    "# Define feature columns\n",
    "input_parameters = [\n",
    "    'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_BALANCE', 'AMT_ANNUITY', 'SK_DPD', 'CNT_CHILDREN',\n",
    "    'FLAG_OWN_CAR', 'CODE_GENDER', 'DAYS_CREDIT', 'DAYS_DECISION', 'AMT_PAYMENT', \n",
    "    'AMT_INSTALMENT', 'AMT_APPLICATION', 'NAME_FAMILY_STATUS', 'NAME_INCOME_TYPE', \n",
    "    'NAME_HOUSING_TYPE', 'NAME_CONTRACT_TYPE'\n",
    "]\n",
    "\n",
    "# Filter columns available in merged_data\n",
    "available_columns = [col for col in input_parameters if col in merged_data.columns]\n",
    "\n",
    "# Print available columns for debugging\n",
    "print(\"Available columns:\", available_columns)\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = [\n",
    "    'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_BALANCE', 'AMT_ANNUITY', 'SK_DPD', \n",
    "    'CNT_CHILDREN'\n",
    "]\n",
    "categorical_features = ['NAME_FAMILY_STATUS', 'NAME_INCOME_TYPE', 'NAME_HOUSING_TYPE', 'NAME_CONTRACT_TYPE']\n",
    "\n",
    "# Split data into features and target\n",
    "training_data = merged_data[available_columns + ['TARGET']]\n",
    "X = training_data[available_columns]\n",
    "y = training_data['TARGET']\n",
    "\n",
    "# Define the preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create a full pipeline that includes preprocessing and the model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', CatBoostClassifier(silent=True, random_state=42))\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Save the pipeline\n",
    "save_dir = 'C:\\\\Users\\\\SMRUTI DESHPANDE\\\\house credit default\\\\'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "joblib.dump(pipeline, os.path.join(save_dir, 'credit_model_pipeline.pkl'))\n",
    "\n",
    "print(\"Pipeline saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29bbaba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
